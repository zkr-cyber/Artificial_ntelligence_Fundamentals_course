#!/usr/bin/env python3
"""
äº‘æœåŠ¡å™¨è®­ç»ƒè„šæœ¬ - é’ˆå¯¹ RTX 5090 ä¼˜åŒ–
é€‚é… PyTorch 2.8.0 + CUDA 12.8
"""

import sys
import subprocess
from pathlib import Path
import torch


def check_cloud_environment():
    """æ£€æŸ¥äº‘æœåŠ¡å™¨ç¯å¢ƒ"""
    print("="*60)
    print("äº‘æœåŠ¡å™¨ç¯å¢ƒæ£€æŸ¥")
    print("="*60)
    
    # æ£€æŸ¥PyTorch
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # æ£€æŸ¥CUDA
    if torch.cuda.is_available():
        print(f"âœ“ CUDAå¯ç”¨: {torch.version.cuda}")
        print(f"âœ“ GPUæ•°é‡: {torch.cuda.device_count()}")
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    else:
        print("âœ— CUDAä¸å¯ç”¨")
        sys.exit(1)
    
    # æ£€æŸ¥ultralytics
    try:
        import ultralytics
        print(f"âœ“ Ultralyticsç‰ˆæœ¬: {ultralytics.__version__}")
    except ImportError:
        print("âœ— æœªå®‰è£…ultralytics")
        print("  å®‰è£…ä¸­...")
        subprocess.run([sys.executable, "-m", "pip", "install", "ultralytics"])
    
    print("="*60 + "\n")
    
    return torch.cuda.device_count()


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="äº‘æœåŠ¡å™¨è®­ç»ƒå¯åŠ¨å™¨ï¼ˆRTX 5090ä¼˜åŒ–ï¼‰")
    
    # è®­ç»ƒæ¨¡å¼
    parser.add_argument("--mode", type=str, default="full",
                       choices=["test", "standard", "full", "medical"],
                       help="è®­ç»ƒæ¨¡å¼")
    parser.add_argument("--data", type=str, 
                       default="configs/ma_seg_all.yaml",
                       help="æ•°æ®é›†é…ç½®æ–‡ä»¶")
    parser.add_argument("--name", type=str, default=None,
                       help="å®éªŒåç§°")
    
    # RTX 5090 ä¼˜åŒ–å‚æ•°
    parser.add_argument("--batch", type=int, default=None,
                       help="æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤æ ¹æ®æ¨¡å¼è‡ªåŠ¨è®¾ç½®ï¼‰")
    parser.add_argument("--imgsz", type=int, default=640,
                       help="å›¾åƒå°ºå¯¸")
    parser.add_argument("--workers", type=int, default=16,
                       help="æ•°æ®åŠ è½½çº¿ç¨‹æ•°ï¼ˆRTX 5090å»ºè®®16+ï¼‰")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--epochs", type=int, default=None,
                       help="è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤æ ¹æ®æ¨¡å¼è‡ªåŠ¨è®¾ç½®ï¼‰")
    parser.add_argument("--patience", type=int, default=None,
                       help="æ—©åœè€å¿ƒå€¼ï¼ˆé»˜è®¤æ ¹æ®æ¨¡å¼è‡ªåŠ¨è®¾ç½®ï¼‰")
    
    # é«˜çº§é€‰é¡¹
    parser.add_argument("--amp", action="store_true", default=True,
                       help="å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰è®­ç»ƒ")
    parser.add_argument("--cache", type=str, default="ram",
                       choices=["False", "disk", "ram"],
                       help="ç¼“å­˜ç­–ç•¥ï¼ˆRTX 5090å»ºè®®ramï¼‰")
    parser.add_argument("--resume", type=str, default=None,
                       help="ä»æ£€æŸ¥ç‚¹æ¢å¤è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # ç¯å¢ƒæ£€æŸ¥
    gpu_count = check_cloud_environment()
    
    # æ ¹æ®æ¨¡å¼é…ç½®å‚æ•°
    if args.mode == "test":
        print("ğŸ“ æ¨¡å¼: å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯ç¯å¢ƒï¼‰")
        epochs = args.epochs or 10
        batch = args.batch or 32
        patience = args.patience or 10
        name = args.name or "cloud_test"
        optimizer_config = {}
        
    elif args.mode == "standard":
        print("ğŸ“ æ¨¡å¼: æ ‡å‡†è®­ç»ƒ")
        epochs = args.epochs or 200
        batch = args.batch or 32
        patience = args.patience or 50
        name = args.name or "cloud_standard"
        optimizer_config = {
            "--optimizer": "AdamW",
            "--lr0": "0.001",
            "--weight_decay": "0.0005",
        }
        
    elif args.mode == "full":
        print("ğŸ“ æ¨¡å¼: å®Œæ•´è®­ç»ƒï¼ˆRTX 5090ä¼˜åŒ–ï¼‰")
        epochs = args.epochs or 300
        batch = args.batch or 40  # RTX 5090 å¯ä»¥å¤„ç†æ›´å¤§æ‰¹æ¬¡
        patience = args.patience or 80
        name = args.name or "cloud_full_rtx5090"
        optimizer_config = {
            "--optimizer": "AdamW",
            "--lr0": "0.001",
            "--lrf": "0.01",
            "--weight_decay": "0.001",
            "--warmup_epochs": "10",
        }
        
    elif args.mode == "medical":
        print("ğŸ“ æ¨¡å¼: åŒ»å­¦å›¾åƒä¼˜åŒ–ï¼ˆRTX 5090 + é«˜çº§å¢å¼ºï¼‰")
        epochs = args.epochs or 300
        batch = args.batch or 32
        patience = args.patience or 100
        name = args.name or "cloud_medical_rtx5090"
        optimizer_config = {
            "--optimizer": "AdamW",
            "--lr0": "0.0005",
            "--lrf": "0.01",
            "--weight_decay": "0.001",
            "--warmup_epochs": "15",
            "--hsv_h": "0.01",
            "--hsv_s": "0.5",
            "--hsv_v": "0.3",
            "--degrees": "5",
            "--mosaic": "0.8",
            "--flipud": "0.5",
            "--close_mosaic": "30",
            "--mixup": "0.1",
        }
    
    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        sys.executable,
        "scripts/train_improved.py",
        "--data", args.data,
        "--model", "yolo11n-seg.pt",
        "--epochs", str(epochs),
        "--batch", str(batch),
        "--imgsz", str(args.imgsz),
        "--device", "cuda",
        "--patience", str(patience),
        "--name", name,
        "--workers", str(args.workers),
        "--cache", args.cache,
    ]
    
    # æ·»åŠ  AMP
    if args.amp:
        cmd.extend(["--amp", "True"])
    
    # æ·»åŠ ä¼˜åŒ–å™¨é…ç½®
    for key, value in optimizer_config.items():
        cmd.extend([key, value])
    
    # æ¢å¤è®­ç»ƒ
    if args.resume:
        cmd.extend(["--resume", args.resume])
    
    # æ‰“å°é…ç½®
    print("="*60)
    print("è®­ç»ƒé…ç½®ï¼ˆRTX 5090 ä¼˜åŒ–ï¼‰")
    print("="*60)
    print(f"æ•°æ®é›†: {args.data}")
    print(f"GPU: CUDA (RTX 5090)")
    print(f"è®­ç»ƒè½®æ•°: {epochs}")
    print(f"æ‰¹æ¬¡å¤§å°: {batch} (å……åˆ†åˆ©ç”¨æ˜¾å­˜)")
    print(f"å›¾åƒå°ºå¯¸: {args.imgsz}")
    print(f"å·¥ä½œçº¿ç¨‹: {args.workers}")
    print(f"ç¼“å­˜ç­–ç•¥: {args.cache}")
    print(f"æ··åˆç²¾åº¦: {'å¯ç”¨' if args.amp else 'ç¦ç”¨'}")
    print(f"æ—©åœè€å¿ƒå€¼: {patience}")
    print(f"å®éªŒåç§°: {name}")
    print("="*60 + "\n")
    
    # æ˜¾ç¤ºå‘½ä»¤
    print("æ‰§è¡Œå‘½ä»¤:")
    print(" ".join(cmd))
    print("\n" + "="*60 + "\n")
    
    # å¼€å§‹è®­ç»ƒï¼ˆè‡ªåŠ¨è¿è¡Œï¼Œæ— éœ€ç¡®è®¤ï¼‰
    print("å¼€å§‹è®­ç»ƒ...")
    print("="*60 + "\n")
    
    try:
        result = subprocess.run(cmd)
        if result.returncode == 0:
            print("\n" + "="*60)
            print("âœ“ è®­ç»ƒå®Œæˆï¼")
            print("="*60)
            
            # è®­ç»ƒç»“æœè·¯å¾„
            best_model = f"runs/improved_seg/{name}/weights/best.pt"
            print(f"\næœ€ä½³æ¨¡å‹: {best_model}")
            print("\nä¸‹ä¸€æ­¥:")
            print(f"1. è¯„ä¼°æ¨¡å‹:")
            print(f"   python scripts/eval_improved.py --models {best_model} --device cuda")
            print(f"\n2. ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°:")
            print(f"   scp -P 34066 root@connect.bjb2.seetacloud.com:/root/ma_seg_project/{best_model} .")
            print("="*60)
        else:
            print("\n" + "="*60)
            print("âœ— è®­ç»ƒå¤±è´¥")
            print("="*60)
            sys.exit(1)
    except KeyboardInterrupt:
        print("\n\nè®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\né”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()

